<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RAG on Nixum Blog</title>
    <link>http://nixum.cc/tags/rag/</link>
    <description>Recent content in RAG on Nixum Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://nixum.cc/tags/rag/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>向量数据库在 RAG 中的应用</title>
      <link>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9C%A8-rag-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9C%A8-rag-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>[TOC]
一、从场景出发 虽然目前ChatGPT等大语言模型已经十分好用了，无论是响应速度和回答的质量，基本上能解决我们日常一些问题和简单的工作，但不可否认，目前的大语言模型仍然有很多缺陷，比如：
 回答幻觉：大语言模型回答问题的本质上是基于其已有的训练数据，预测(概率计算)出哪些可能的文字作为答案，所以难免会出现张冠李戴、胡说八道的情况，特别是在大模型不擅长的领域，最典型的比如你在 ChatGPT-3.5问他“西红柿炒钢丝球要怎么做“，它会十分正经的回答出让人哭笑不得的答案，又或者问一些代码问题，它有时会回答出一些不存在的语法或者方法的调用，产生不正确的答案； 上下文限制：比如Chat GPT-3.5 Turbo的上下文限制是4K tokens（大概3000字），GPT-4 Turbo的上下文限制是128K tokens（大概9.6万字），这意味着其最多只能处理（记忆）这么多字的内容，且随着处理的上下文越多，响应速度也会越来越慢，成本越来越高； 训练的语料更新不够及时：比如Chat GPT-3.5 Turbo训练的语料库只记录了2021年9月之前的数据，GPT-4 Turbo则是2023年4月，这意味着在此之后产生的数据模型是不知道的； 在某些领域还不够专业：比如某些垂直领域的训练语料往往比较封闭，不对外公开，又或者是企业的内部数据，处于对数据的安全性的考量，并不希望上传到第三方平台进行训练，大模型无法获取这些数据进行训练，此时只依赖通用大模型的能力，回答的质量就会大打折扣；  为了优化上述问题，提升大语言模型回答的质量，其中一种解决方案就是外挂一个知识库，在提问时先根据问题，检索出相关更加准确且核心的资料，指导大语言模型生成更加准确的答案。
二、RAG的基本原理 下面是一个最简单的RAG基本流程：
 
阶段一  结构化数据并进行文本分割：将大量文档（可能是pdf、word、文本、网页等等）进行结构化，统一数据结构，分割成多个文本块； 将文本块向量化：使用Embedding模型，将分割后的文本块转换成向量，通过向量将不同文本块进行关联； 存入向量数据库：将向量以及对应的文本块（元数据），选择合适的算法，存入到向量数据库中；  阶段二  用户提问向量化：用户提出问题时，使用阶段一中相同的Embedding模型，将问题转成向量； 检索召回：将问题转换后的向量，在向量数据库中进行检索，选择合适的算法，计算向量间的距离，得到与之相似的向量以及对应的文本块； 提示词增强：将用户的问题以及上一步检索到的文本数据，进行提示词优化，构建出最终的提示词，发送给大模型，由大模型生成最终的结果并返回；  核心步骤 在上面整个流程中，有几个核心步骤决定了最终RAG的质量，包括后续的优化，也是从这三个步骤入手：
  文本的处理和索引：如何更好的把文本数据存起来
文本分割的目的，一个是因为解决大模型输入长度的限制，另一个在保持语义连贯的同时，减少嵌入内容的噪声，更加有效的检索到用户问题更相关的文本资料；
怎么分割是一个取舍的问题，如果分块太大，可能会导致分块包含了太多信息，降低检索的准确性，分块太小又可能会丢失必要的上下文信息，导致最终生成的回答缺乏连贯性或深度；
分割后的文本最终需要被检索，因此需要将文本转换为向量，这也依赖embedding模型的能力，而embedding模型的训练语料、参数的数量级，决定了转换出来的向量的关联性，最终影响文本间的语义相似度；
  检索召回：如何在大量的文本数据中，找到一小部分有用的数据，给到模型参考
向量和对应文本的存储和检索，又依赖向量数据库，需要解决不同数量级维度的向量要如何存储，如何才能快速计算其相似度，快速精确的定位和检索数据的问题，甚至为了进一步提升检索的质量，除了需要提供相似度检索，还需要提供传统的关键字检索等；
单纯的向量召回存在精度问题，因此可能需要多种召回的方式，比如分词召回、图谱召回等，对于召回出来的数据，可能还需要进一步的处理，进行各种去重、合并和重排等，检索出到更加精确的数据；
  内容的生成：如何让大模型生成更有用的答案
通过提示词优化，指导大模型如何利用这些检索出来的数据，如何排除无关的数据，如何更好的回答问题等；
   
三、为什么要使用向量数据库 或许你可能会疑惑，如果用传统数据库或者es等搜索出关联的信息，再跟着问题一起发送给大语言模型，也能实现类似的效果，这样行不行？答案当然是可以，但它不是最优的，出来的效果也并不好，原因在于传统数据库的搜索功能都是基于关键字搜索，只能匹配出对应的文本，语义上的联系其实非常弱。
传统数据库都是基于B+树或者分词+倒排索引的方式进行关键字匹配和排序，得到最终结果，例如，通过传统数据库搜索”布偶猫“，只能匹配得到带有”布偶猫“这个关键字相关的结果，无法得到”银渐层“、”蓝猫“等结果，因为他们是不同的词，传统数据库无法识别他们的语义关系。
 
而向量数据库是基于向量搜索的，需要我们事先将”蓝猫“，”银渐层“，”布偶“，根据他们的特征比如大小、毛发长短、颜色、习性、脸型等维度，计算出一组数字后作为他们的代表进行存储（这组数字也被称为向量），只要分解的维度足够多，就能将所有猫区分出来，然后通过计算向量间的距离来判断他们的相似度，产生语义上的联系；
 
向量数据库并不是什么特别新的技术，早在机器学习场景中就有广泛应用，比如人脸识别、以图搜图、音乐软件的听音识曲等都有应用到，只是最近被大模型带火了一把。
 向量数据库用专门的数据结构和算法来处理向量之间的相似性计算和查询。 通过构建索引结构，向量数据库可以快速找到最相似的向量，以满足各种应用场景中的查询需求。
 上面是AWS上找到的对向量数据库的描述，在RAG的场景下，向量数据库的核心作用，就是将用户准备好的强相关性的文本转成向量，存储到数据库中，当用户输入问题时，也将问题转成向量，然后在数据库中进行相似性搜索，找到相关联的向量和上下文，进而找到对应的文本，最后跟着问题一起发送给大语言模型，从而达到减少模型的计算量，提升理解能力和响应速度，降低成本，绕过tokens限制，提高回答质量的目的。</description>
    </item>
    
  </channel>
</rss>
