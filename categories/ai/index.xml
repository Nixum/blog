<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Nixum Blog</title>
    <link>http://nixum.cc/categories/ai/</link>
    <description>Recent content in AI on Nixum Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://nixum.cc/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>向量数据库在 RAG 中的应用</title>
      <link>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9C%A8-rag-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9C%A8-rag-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid>
      <description>[TOC]
一、理解几个基本概念 https://www.bilibili.com/video/BV1CVAUeuECE?spm_id_from=333.788.videopod.sections&amp;vd_source=a870d050aec1403366ebea0f42b4cdaf
https://www.bilibili.com/video/BV1smXUYSEGi/?spm_id_from=333.1387.upload.video_card.click&amp;vd_source=a870d050aec1403366ebea0f42b4cdaf
https://www.bilibili.com/video/BV1bfoQYCEHC/?spm_id_from=333.1387.upload.video_card.click&amp;vd_source=a870d050aec1403366ebea0f42b4cdaf
https://www.bilibili.com/video/BV1CpAnebEpJ/?spm_id_from=333.1387.upload.video_card.click&amp;vd_source=a870d050aec1403366ebea0f42b4cdaf
二、GPT的局限性 虽然目前ChatGPT等大语言模型已经十分好用了，无论是响应速度和回答的质量，基本上能解决我们日常一些问题和简单的工作，但不可否认，目前的大语言模型仍然有很多缺陷，比如：
  回答幻觉：大语言模型回答问题的本质上是基于其已有的训练数据，预测(概率计算)出哪些可能的文字作为答案，所以难免会出现张冠李戴、胡说八道的情况，特别是在大模型不擅长的领域。
最典型的比如你在 ChatGPT-3.5问他“西红柿炒钢丝球要怎么做“，它会十分正经的回答出让人哭笑不得的答案，又或者问一些代码问题，它有时会回答出一些不存在的语法或者方法的调用，产生不正确的答案；
  上下文限制：由于硬件限制和模型底层架构限制，上下文越长，计算时，其内存需求、计算量等会剧增，硬件资源难以负荷，另外，Transformer架构在处理长序列时，如果信息距离太远，会出现信息稀释或丢失，导致生成的内容连贯性和准确性降低，因此大模型都会限制上下文的长度，以保证生成的结果。
比如Chat GPT-3.5 Turbo的上下文限制是4K tokens（大概3000字），GPT-4 Turbo的上下文限制是128K tokens（大概9.6万字），这意味着其最多只能处理（记忆）这么多字的内容，且随着处理的上下文越多，响应速度也会越来越慢，成本越来越高；
  训练的语料更新不够及时：比如Chat GPT-3.5 Turbo训练的语料库只记录了2021年9月之前的数据，GPT-4 Turbo则是2023年4月，这意味着在此之后产生的数据模型是不知道的；
  在某些领域还不够专业：比如某些垂直领域的训练语料往往比较封闭，不对外公开，又或者是企业的内部数据，处于对数据的安全性的考量，并不希望上传到第三方平台进行训练，大模型无法获取这些数据进行训练，此时只依赖通用大模型的能力，回答的质量就会大打折扣；
  为了优化上述问题，提升大语言模型回答的质量，其中一种解决方案就是外挂一个知识库，在提问时先根据问题，检索出相关更加准确且核心的资料，指导大语言模型生成更加准确的答案。
二、RAG的基本原理 下面是一个最简单的RAG基本流程：
 
阶段一  结构化数据并进行文本分割：将大量文档（可能是pdf、word、文本、网页等等）进行结构化，统一数据结构，分割成多个文本块； 将文本块向量化：使用Embedding模型，将分割后的文本块转换成向量，通过向量将不同文本块进行关联； 存入向量数据库：将向量以及对应的文本块（元数据），选择合适的算法，存入到向量数据库中；  阶段二  用户提问向量化：用户提出问题时，使用阶段一中相同的Embedding模型，将问题转成向量； 检索召回：将问题转换后的向量，在向量数据库中进行检索，选择合适的算法，计算向量间的距离，得到与之相似的向量以及对应的文本块； 提示词增强：将用户的问题以及上一步检索到的文本数据，进行提示词优化，构建出最终的提示词，发送给大模型，由大模型生成最终的结果并返回；  核心步骤 在上面整个流程中，有几个核心步骤决定了最终RAG的质量，包括后续的优化，也是从这三个步骤入手：
  文本的处理和索引：如何更好的把文本数据存起来
文本分割的目的，一个是因为解决大模型输入长度的限制，另一个在保持语义连贯的同时，减少嵌入内容的噪声，更加有效的检索到用户问题更相关的文本资料；
怎么分割是一个取舍的问题，如果分块太大，可能会导致分块包含了太多信息，降低检索的准确性，分块太小又可能会丢失必要的上下文信息，导致最终生成的回答缺乏连贯性或深度；
分割后的文本最终需要被检索，因此需要将文本转换为向量，这也依赖embedding模型的能力，而embedding模型的训练语料、参数的数量级，决定了转换出来的向量的关联性，最终影响文本间的语义相似度；
  检索与召回：如何在大量的文本数据中，找到一小部分有用的数据，给到模型参考
向量和对应文本的存储和检索，又依赖向量数据库，需要解决不同数量级维度的向量要如何存储，如何才能快速计算其相似度，快速精确的定位和检索数据的问题，甚至为了进一步提升检索的质量，除了需要提供相似度检索，还需要提供传统的关键字检索等；
单纯的向量召回存在精度问题，因此可能需要多种召回的方式，比如分词召回、图谱召回等，对于召回出来的数据，可能还需要进一步的处理，进行各种去重、合并和重排等，检索出到更加精确的数据；
  内容的生成：如何让大模型生成更有用的答案
通过提示词优化，指导大模型如何利用这些检索出来的数据，如何排除无关的数据，如何更好的回答问题等；</description>
    </item>
    
    <item>
      <title>AI基础概念整理</title>
      <link>http://nixum.cc/p/ai%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/</link>
      <pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate>
      
      <guid>http://nixum.cc/p/ai%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/</guid>
      <description>基础概念 1. 模型的参数  
参数是什么？
 大模型就像一个超级复杂的“公式 + 神经网络大脑”。 里面有无数个可以调整的“小旋钮”，每个旋钮的数值就是一个“参数”。 每个参数都是一个数字（通常是 16 或 32 位浮点数），它们本身是没有任何含义的。 “70B 参数模型” = 有 700 亿个这样的数字参与计算。  这些参数本质上就是“模型记住的东西”和“模型如何变换信息的方式”。
参数是怎么来的？
参数是训练出来的，不是人为写的，训练的时候只知道输入和输出，通过训练后得到参数。
 
 一开始参数是随机的 → 模型啥都不会。 通过“预测下一个词/下一个 token”这个任务，不断修正参数。 训练规模越大、数据越丰富，模型越“聪明”。 我们常说模型开源，开源的就是训练后的参数，训练过程一般是不开源的。  2. Token  Token 也叫词元，是模型眼中“最小的语义单位”，是模型处理文本的基本粒度。不是严格意义的“字”或“词”，而是由 tokenizer 定义的一种切分方式。  &amp;ldquo;Hello&amp;rdquo; → 可能是一个 token；&amp;ldquo;incredible&amp;rdquo; → 可能被拆成 &amp;quot;in&amp;quot;, &amp;quot;cred&amp;quot;, &amp;quot;ible&amp;quot; 三个 token； 有的模型：一字一 token：“你”“好”“吗”；有的模型会把常用词合并，例如“人工智能”可能是 1~2 个 token。   Tokenizer：把文本切分成 token，再将 token 映射成 id，方便在后续的处理时，把 id 再映射成向量。因为模型只能处理数字，所以需要先将文本转换成数字。 Token 的类型：  prompt_tokens：输入的 token completion_tokens：输出的 token total_tokens：总 token context：上下文，包含了输入和输出 和 历史对话 KV 缓存：是模型内部的机制（自回归生成时的优化技术），作用于单次对话的生成阶段。  比如：在对话里输入 1000 个 token，LLM 需要对这 1000 个 token 一次性建模，计算注意力，产生很多 KV，这些 KV 会被缓存，生成第一个 token 时，LLM 只需要对新生成的 token 做计算，再利用上一轮缓存的结果进行重新的计算即可，无需重新重头计算对话里的 1001 个 token。   提示词缓存：是 推理服务层面的优化，不属于 LLM 的内部机制，缓存的是输入 prompt 的处理结果，作用于跨请求的优化，取决于 API 提供方的实现。  比如：第一次对话输入 800(system_prompt) + 200(user_prompt) token，LLM 回答 100 (assistant_prompt) token，第二次对话输入 800(system_prompt) + 200(user_prompt) + 100 (assistant_prompt) token + 50 (user_prompt) token ，此时会缓存 800 + 200 个 token     Prompt 的类型：  system_prompt：系统提示词，通常只会有一个，不是必需的，但有助于设定 assistant 的整体行为，帮助模型了解用户的需求，并根据这些需求提供相应的响应 user_prompt：用户提示词，用户输入的内容 assistant_prompt：LLM 回答的提示词，可以携带工具的调用信息    3.</description>
    </item>
    
  </channel>
</rss>
